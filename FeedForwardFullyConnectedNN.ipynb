{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTobZ1KaTv1k"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from operator import add\n",
        "from scipy.special import expit\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import pickle as pk\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVyxeiBsT0R0"
      },
      "outputs": [],
      "source": [
        "class MyNeuralNetwork():\n",
        "    \"\"\"\n",
        "    My implementation of a Neural Network Classifier.\n",
        "    \"\"\"\n",
        "\n",
        "    acti_fns = ['relu', 'sigmoid', 'linear', 'tanh', 'softmax']\n",
        "    weight_inits = ['zero', 'random', 'normal']\n",
        "\n",
        "    def __init__(self, n_layers, layer_sizes, activation, learning_rate, weight_init, batch_size, num_epochs):\n",
        "        \"\"\"\n",
        "        Initializing a new MyNeuralNetwork object\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_layers : int value specifying the number of layers\n",
        "\n",
        "        layer_sizes : integer array of size n_layers specifying the number of nodes in each layer\n",
        "\n",
        "        activation : string specifying the activation function to be used\n",
        "                     possible inputs: relu, sigmoid, linear, tanh\n",
        "\n",
        "        learning_rate : float value specifying the learning rate to be used\n",
        "\n",
        "        weight_init : string specifying the weight initialization function to be used\n",
        "                      possible inputs: zero, random, normal\n",
        "\n",
        "        batch_size : int value specifying the batch size to be used\n",
        "\n",
        "        num_epochs : int value specifying the number of epochs to be used\n",
        "        \"\"\"\n",
        "\n",
        "        self.n_layers = n_layers\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.activation = activation\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_init = weight_init\n",
        "        self.batch_size = batch_size\n",
        "        self.num_epochs = num_epochs\n",
        "\n",
        "        if activation not in self.acti_fns:\n",
        "            raise Exception('Incorrect Activation Function')\n",
        "\n",
        "        if weight_init not in self.weight_inits:\n",
        "            raise Exception('Incorrect Weight Initialization Function')\n",
        "        #pass\n",
        "\n",
        "    def compare_with_mlp(self,loss_from_your_model,X,y):\n",
        "        # comparing with MLP\n",
        "        # X is all data and Y is all labels\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
        "\n",
        "        self.mlp = MLPClassifier(hidden_layer_sizes=(256, 128, 64), activation ='relu',solver = 'sgd' , alpha = 0, batch_size = 1,max_iter=25, learning_rate_init = 0.01, learning_rate = 'constant',shuffle=True,momentum = 0,nesterovs_momentum=False,validation_fraction = 0.1)\n",
        "\n",
        "        self.mlp.fit(X_train, y_train)\n",
        "\n",
        "        self.output = self.mlp.score(X_test, y_test)\n",
        "        print(self.output)\n",
        "\n",
        "        loss_from_sklearn = self.mlp.loss_curve_\n",
        "        print(loss_from_sklearn)\n",
        "\n",
        "        plt.plot(loss_from_sklearn,label=\"sklearn\")\n",
        "        plt.plot(loss_from_your_model,label=\"your NN\")\n",
        "        plt.legend(loc=\"upper left\")\n",
        "        plt.savefig(\"result.png\")\n",
        "        plt.close()\n",
        "\n",
        "    def relu(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the ReLU activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array [zl = wlal-1 + bl]\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X [al]\n",
        "        \"\"\"\n",
        "\n",
        "        X_calc = np.maximum(0.,X)\n",
        "        return X_calc\n",
        "\n",
        "    def relu_grad(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the gradient of ReLU activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array [zl]\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X [zl']\n",
        "        \"\"\"\n",
        "\n",
        "        X_calc = np.greater(X, 0.).astype(np.float64)\n",
        "        return X_calc\n",
        "\n",
        "    def sigmoid(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the Sigmoid activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "\n",
        "        X_calc = 1.0/(1+np.exp(-X))\n",
        "        return X_calc\n",
        "\n",
        "    def sigmoid_grad(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the gradient of Sigmoid activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        s = self.sigmoid(X)\n",
        "        X_calc = s*(1-s)\n",
        "        return X_calc\n",
        "\n",
        "    def linear(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the Linear activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "\n",
        "        X_calc = X.copy()\n",
        "        return X_calc\n",
        "\n",
        "    def linear_grad(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the gradient of Linear activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "\n",
        "        X_calc = np.ones(len(X))\n",
        "        return X_calc\n",
        "\n",
        "    def tanh(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the Tanh activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "\n",
        "        X_calc = (np.exp(X)-np.exp(-X))/(np.exp(X)+np.exp(-X))\n",
        "        return X_calc\n",
        "\n",
        "    def tanh_grad(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the gradient of Tanh activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        t = self.tanh(X)\n",
        "        X_calc = 1-t**2\n",
        "        return X_calc\n",
        "\n",
        "    def softmax(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the ReLU activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "\n",
        "        #denom1 = np.sum([np.exp(x) for x in X])\n",
        "        #X_calc = [np.exp(x)/denom1 for x in X]\n",
        "        e_x = np.exp(X)\n",
        "        return e_x / e_x.sum()\n",
        "        #return np.array(X_calc)\n",
        "\n",
        "    def softmax_grad(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the gradient of Softmax activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 2-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"sm = self.softmax(X)\n",
        "        X_calc = np.zeros((len(X), len(X)))\n",
        "        for i in range(0, len(X)):\n",
        "          for j in range(0, len(X)):\n",
        "            if (i==j):\n",
        "              X_calc[i][j] = sm[i]*(1-sm[i])\n",
        "            else:\n",
        "              X_calc[i][j] = -1*sm[i]*sm[j]\"\"\"\n",
        "        soft = self.softmax(X)\n",
        "        s = soft.reshape(-1,1)\n",
        "        return np.diagflat(s) - np.dot(s, s.T)\n",
        "        #return X_calc\n",
        "\n",
        "    def zero_init(self, shape):\n",
        "        \"\"\"\n",
        "        Calculating the initial weights after Zero Activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        shape : tuple specifying the shape of the layer for which weights have to be generated  nl-1 x nl\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        weight : 2-dimensional numpy array which contains the initial weights for the requested layer\n",
        "        \"\"\"\n",
        "\n",
        "        weight_calc = np.zeros((shape[1], shape[0]))\n",
        "        return weight_calc\n",
        "\n",
        "    def random_init(self, shape):\n",
        "        \"\"\"\n",
        "        Calculating the initial weights after Random Activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        shape : tuple specifying the shape of the layer for which weights have to be generated\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        weight : 2-dimensional numpy array which contains the initial weights for the requested layer\n",
        "        \"\"\"\n",
        "\n",
        "        weight_calc = np.random.uniform(-1,1,(shape[1],shape[0])) * 0.01\n",
        "        return weight_calc\n",
        "\n",
        "    def normal_init(self, shape):\n",
        "        \"\"\"\n",
        "        Calculating the initial weights after Normal(0,1) Activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        shape : tuple specifying the shape of the layer for which weights have to be generated\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        weight : 2-dimensional numpy array which contains the initial weights for the requested layer\n",
        "        \"\"\"\n",
        "\n",
        "        weight_calc = np.random.normal(size=[shape[1], shape[0]]) * 0.01\n",
        "        return weight_calc\n",
        "\n",
        "    def valloss(self, X, Y):\n",
        "        \"\"\"\n",
        "        Predicting probabilities using the trained linear model.\n",
        "​\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
        "​\n",
        "        Returns\n",
        "        -------\n",
        "        y : 2-dimensional numpy array of shape (n_samples, n_classes) which contains the\n",
        "            class wise prediction probabilities.\n",
        "        \"\"\"\n",
        "        val_loss = 0.0\n",
        "        x_input = X.copy()\n",
        "        y_input = np.zeros((len(X), self.num_classes))\n",
        "        for i in range(0, len(X)):\n",
        "          ind = Y[i]\n",
        "          y_input[i][int(ind)] = 1\n",
        "        #print(pred_prob)\n",
        "        for i in range(0, len(X)):\n",
        "          #layer 0\n",
        "          self.a[0] = x_input[i].copy()\n",
        "          # FORWARD PASS\n",
        "          for l in range(1, self.n_layers):\n",
        "            self.z[l] = np.dot(self.weights[l], self.a[l-1])\n",
        "            if (self.activation=='relu'):\n",
        "              self.a[l] = self.relu(self.z[l])\n",
        "            elif (self.activation=='sigmoid'):\n",
        "              self.a[l] = self.sigmoid(self.z[l])\n",
        "            elif (self.activation=='linear'):\n",
        "              self.a[l] = self.linear(self.z[l])\n",
        "            elif (self.activation=='tanh'):\n",
        "              self.a[l] = self.tanh(self.z[l])\n",
        "            elif (self.activation=='softmax'):\n",
        "              self.a[l] = self.softmax(self.z[l])\n",
        "            else:\n",
        "              print(\"activation fun not valid!\")\n",
        "\n",
        "          self.z[self.n_layers] = np.dot(self.weights[self.n_layers], self.a[self.n_layers-1])\n",
        "          self.a[self.n_layers] = self.softmax(self.z[self.n_layers])\n",
        "\n",
        "          for j in range(0, 10):\n",
        "            if (y_input[i][j]==1):\n",
        "              break\n",
        "          val_loss += np.log2(self.a[self.n_layers][j])\n",
        "\n",
        "        # return the numpy array y which contains the predicted values\n",
        "        return val_loss\n",
        "\n",
        "    def fit(self, X, y, Xval, Yval):\n",
        "        \"\"\"\n",
        "        Fitting (training) the linear model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
        "\n",
        "        y : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : an instance of self\n",
        "        \"\"\"\n",
        "        self.train_loss = {}\n",
        "        self.val_loss = {}\n",
        "        count = 0\n",
        "\n",
        "        for i in range(0,26):\n",
        "          self.train_loss[i]=0.0\n",
        "          self.val_loss[i]=0.0\n",
        "\n",
        "        #step-0 - convert y to 1 hot encoded\n",
        "        self.num_classes = 10 #10\n",
        "        x_input = X.copy()\n",
        "        #y_input = y.copy()\n",
        "        y_input = np.zeros((len(X), self.num_classes))\n",
        "        for i in range(0, len(X)):\n",
        "          ind = y[i]\n",
        "          y_input[i][int(ind)] = 1\n",
        "\n",
        "        #INIT - x_input, y_input [one hot encoded]\n",
        "        self.weights = {}\n",
        "        self.bias = {}\n",
        "        self.a = {}\n",
        "        self.z = {}\n",
        "\n",
        "        #layer 0 - input layer\n",
        "        self.layer_sizes = np.insert(self.layer_sizes, 0, len(x_input[0]))\n",
        "\n",
        "        #layer l - [1,#layers]\n",
        "        for layer in range(1,self.n_layers+1):\n",
        "          if (self.weight_init=='zero'):\n",
        "            temp = [self.layer_sizes[layer-1], self.layer_sizes[layer]]\n",
        "            self.weights[layer] = self.zero_init(temp)\n",
        "          elif (self.weight_init=='random'):\n",
        "            self.weights[layer] = self.random_init([self.layer_sizes[layer-1], self.layer_sizes[layer]])\n",
        "          elif (self.weight_init=='normal'):\n",
        "            self.weights[layer] = self.normal_init([self.layer_sizes[layer-1], self.layer_sizes[layer]])\n",
        "          else:\n",
        "            print(\"weight init not valid!\")\n",
        "          self.bias[layer] = np.zeros(self.layer_sizes[layer])\n",
        "        pk.dump(nn.bias,open('bias'+str(1)+'.pkl','wb'), protocol=pk.HIGHEST_PROTOCOL)\n",
        "        return\n",
        "\n",
        "        \"\"\"for l in range(1, self.n_layers+1):\n",
        "          print(\"layer \"+str(l))\n",
        "          print(self.weights[l].shape)\"\"\"\n",
        "\n",
        "        # MINI BATCH SGD\n",
        "        for epoch in range(1,self.num_epochs+1):\n",
        "          print(\"epoch : \"+str(epoch))\n",
        "          countl = 1\n",
        "          left_indices = [i for i in range(0, len(x_input))]\n",
        "          #random.shuffle(left_indices)\n",
        "          #1500 iterations\n",
        "          while(len(left_indices)>0):\n",
        "            #0. init + random shuffle\n",
        "            self.loss = 0\n",
        "            self.deriv_weights = {}\n",
        "            for l in range(1, self.n_layers+1):\n",
        "              self.deriv_weights[l] = np.zeros((self.layer_sizes[l], self.layer_sizes[l-1]))\n",
        "\n",
        "            #1. randomly sample batch_size samples without replacement\n",
        "            indices = random.sample(left_indices, self.batch_size)\n",
        "            left_indices = [i for i in left_indices if i not in indices]\n",
        "            #random.shuffle(left_indices)\n",
        "\n",
        "            for i in indices:\n",
        "              #layer 0\n",
        "              self.a[0] = x_input[i].copy()\n",
        "              # FORWARD PASS\n",
        "              for l in range(1, self.n_layers):\n",
        "                self.z[l] = np.dot(self.weights[l], self.a[l-1])\n",
        "                if (self.activation=='relu'):\n",
        "                  self.a[l] = self.relu(self.z[l])\n",
        "                elif (self.activation=='sigmoid'):\n",
        "                  self.a[l] = self.sigmoid(self.z[l])\n",
        "                elif (self.activation=='linear'):\n",
        "                  self.a[l] = self.linear(self.z[l])\n",
        "                elif (self.activation=='tanh'):\n",
        "                  self.a[l] = self.tanh(self.z[l])\n",
        "                elif (self.activation=='softmax'):\n",
        "                  self.a[l] = self.softmax(self.z[l])\n",
        "                else:\n",
        "                  print(\"activation fun not valid!\")\n",
        "                #assertions\n",
        "                assert self.z[l].ndim==1\n",
        "                assert self.a[l].ndim==1\n",
        "                assert(len(self.a[l])==self.layer_sizes[l])\n",
        "                assert(len(self.z[l])==self.layer_sizes[l])\n",
        "                #----------\n",
        "\n",
        "              self.z[self.n_layers] = np.dot(self.weights[self.n_layers], self.a[self.n_layers-1])\n",
        "              self.a[self.n_layers] = self.softmax(self.z[self.n_layers]) #softmax in last layer\n",
        "              #assertions\n",
        "              assert self.z[self.n_layers].ndim==1\n",
        "              assert self.a[self.n_layers].ndim==1\n",
        "              assert(len(self.a[self.n_layers])==self.layer_sizes[self.n_layers])\n",
        "              assert(len(self.z[self.n_layers])==self.layer_sizes[self.n_layers])\n",
        "              #----------\n",
        "              #h_theta(x) = a[lastlayer]\n",
        "              for j in range(0, 10):\n",
        "                if (y_input[i][j]==1):\n",
        "                    break\n",
        "              loss1 = np.log2(self.a[self.n_layers][j])\n",
        "              #self.loss = -1*np.log2(self.a[self.n_layers][y_input[i]]) #update for one hot encoded y\n",
        "\n",
        "              \"\"\"for l in range(1, self.n_layers+1):\n",
        "                print(\"layer \"+str(l))\n",
        "                print(self.a[l].shape)\n",
        "                print(self.z[l].shape)\"\"\"\n",
        "\n",
        "              # BACKWARD PASS\n",
        "              #update rule : w = w - alpha* dJ/dw\n",
        "              self.delta = {}\n",
        "              for layer in range(self.n_layers,0,-1):\n",
        "                if (layer==self.n_layers):\n",
        "                  #base case\n",
        "                  self.delta[layer] = self.a[self.n_layers] - y_input[i]\n",
        "                else:\n",
        "                  if (self.activation=='relu'):\n",
        "                    deriv_af = self.relu_grad(self.z[layer])\n",
        "                  elif (self.activation=='sigmoid'):\n",
        "                    deriv_af = self.sigmoid_grad(self.z[layer])\n",
        "                  elif (self.activation=='linear'):\n",
        "                    deriv_af = self.linear_grad(self.z[layer])\n",
        "                  elif (self.activation=='tanh'):\n",
        "                    deriv_af = self.tanh_grad(self.z[layer])\n",
        "                  elif (self.activation=='softmax'):\n",
        "                    deriv_af = self.softmax_grad(self.z[layer])\n",
        "                  else:\n",
        "                    print(\"activation fun grad not valid!\")\n",
        "\n",
        "                  \"\"\"if self.delta[layer+1].ndim==1:\n",
        "                    temp4 = self.delta[layer+1].reshape(len(self.delta[layer+1]), 1)\n",
        "                  else:\n",
        "                    temp4 = self.delta[layer+1].copy()\"\"\"\n",
        "                  weight_del = np.dot(np.transpose(self.weights[layer+1]), self.delta[layer+1])\n",
        "                  #assertions\n",
        "                  assert self.delta[layer+1].ndim==1, str(self.delta[layer+1].shape)\n",
        "                  #assert deriv_af.ndim==1 #violated in softmax grad\n",
        "                  assert weight_del.ndim==1, str(weight_del.shape)+\" \"+str(layer) #must be 1 d (else @ would be bw 2 matrices (rn bw matrix and vector))\n",
        "                  #---------\n",
        "                  if deriv_af.ndim==1:\n",
        "                    temp6 = deriv_af.reshape(len(deriv_af), 1)\n",
        "                  else:\n",
        "                    temp6 = deriv_af.copy()\n",
        "                  if weight_del.ndim==1:\n",
        "                    temp7 = weight_del.reshape(len(weight_del), 1)\n",
        "                  else:\n",
        "                    temp7 = weight_del.copy()\n",
        "                  if (self.activation=='softmax'):\n",
        "                    self.delta[layer] = temp6 @ temp7\n",
        "                  else:\n",
        "                    self.delta[layer] = np.multiply(temp6, temp7)\n",
        "                  self.delta[layer] = self.delta[layer][:,0]\n",
        "\n",
        "                #store weight gradients\n",
        "                \"\"\"if self.a[layer-1].ndim==1:\n",
        "                  temp2 = self.a[layer-1].reshape(len(self.a[layer-1]),1) #CHANGED added this\n",
        "                else:\n",
        "                  temp2 = self.a[layer-1].copy()\n",
        "                if self.delta[layer].ndim==1:\n",
        "                  temp3 = self.delta[layer].reshape(len(self.delta[layer]),1) #CHANGED added this\n",
        "                else:\n",
        "                  temp3 = self.delta[layer].copy()\"\"\"\n",
        "                self.deriv_weights[layer] += np.dot(self.delta[layer].reshape(len(self.delta[layer]), 1), self.a[layer-1].reshape(1, len(self.a[layer-1]))) #CHANGED TO delta dot at\n",
        "\n",
        "            #update weights, biases\n",
        "            for l in range(1, self.n_layers+1):\n",
        "              self.weights[l] -= (self.learning_rate/self.batch_size) * self.deriv_weights[l]\n",
        "\n",
        "            self.train_loss[count] += loss1\n",
        "            #print(train_loss[epoch]/len(X))\n",
        "            countl += 1\n",
        "            if countl%1000==0:\n",
        "              print(\"count : \"+str(countl))\n",
        "            if countl%9600==0:\n",
        "              #self.val_loss[count] += self.valloss(Xval, Yval)\n",
        "              #pk.dump(nn.weights,open('weights'+str(count)+'.pkl','wb'), protocol=pk.HIGHEST_PROTOCOL)\n",
        "              #--\n",
        "              vl = 0.0\n",
        "              y_input1 = np.zeros((len(X_val), self.num_classes))\n",
        "              for i1 in range(0, len(X_val)):\n",
        "                ind = Y_val[i1]\n",
        "                y_input1[i1][int(ind)] = 1\n",
        "              #print(pred_prob)\n",
        "              for i1 in range(0, len(X_val)):\n",
        "                #layer 0\n",
        "                self.a[0] = X_val[i1].copy()\n",
        "                # FORWARD PASS\n",
        "                for l in range(1, self.n_layers):\n",
        "                  self.z[l] = np.dot(self.weights[l], self.a[l-1])\n",
        "                  if (self.activation=='relu'):\n",
        "                    self.a[l] = self.relu(self.z[l])\n",
        "                  elif (self.activation=='sigmoid'):\n",
        "                    self.a[l] = self.sigmoid(self.z[l])\n",
        "                  elif (self.activation=='linear'):\n",
        "                    self.a[l] = self.linear(self.z[l])\n",
        "                  elif (self.activation=='tanh'):\n",
        "                    self.a[l] = self.tanh(self.z[l])\n",
        "                  elif (self.activation=='softmax'):\n",
        "                    self.a[l] = self.softmax(self.z[l])\n",
        "                  else:\n",
        "                    print(\"activation fun not valid!\")\n",
        "\n",
        "                self.z[self.n_layers] = np.dot(self.weights[self.n_layers], self.a[self.n_layers-1])\n",
        "                self.a[self.n_layers] = self.softmax(self.z[self.n_layers])\n",
        "\n",
        "                for j in range(0, 10):\n",
        "                  if (y_input1[i1][j]==1):\n",
        "                    break\n",
        "                vl += np.log2(self.a[self.n_layers][j])\n",
        "              #--\n",
        "              self.val_loss[count] = vl\n",
        "              print(self.train_loss[count])\n",
        "              print(self.val_loss[count])\n",
        "              count += 1\n",
        "              #print(self.val_loss[count])\n",
        "\n",
        "          #val_loss[count] += self.valloss(Xval, Yval)\n",
        "            #print(val_loss[epoch])\n",
        "\n",
        "            if(count==13 or count==25):\n",
        "              pk.dump(nn.weights,open('weightsrelu_'+str(count)+'.pkl','wb'), protocol=pk.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "        # fit function has to return an instance of itself or else it won't work with test.py\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predicting probabilities using the trained linear model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y : 2-dimensional numpy array of shape (n_samples, n_classes) which contains the\n",
        "            class wise prediction probabilities.\n",
        "        \"\"\"\n",
        "        pred_prob = np.zeros((len(X), self.num_classes))\n",
        "        x_input = X.copy()\n",
        "        #print(pred_prob)\n",
        "        for i in range(0, len(X)):\n",
        "          #layer 0\n",
        "          self.a[0] = x_input[i].copy()\n",
        "          # FORWARD PASS\n",
        "          for l in range(1, self.n_layers):\n",
        "            self.z[l] = np.dot(self.weights[l], self.a[l-1])\n",
        "            if (self.activation=='relu'):\n",
        "              self.a[l] = self.relu(self.z[l])\n",
        "            elif (self.activation=='sigmoid'):\n",
        "              self.a[l] = self.sigmoid(self.z[l])\n",
        "            elif (self.activation=='linear'):\n",
        "              self.a[l] = self.linear(self.z[l])\n",
        "            elif (self.activation=='tanh'):\n",
        "              self.a[l] = self.tanh(self.z[l])\n",
        "            elif (self.activation=='softmax'):\n",
        "              self.a[l] = self.softmax(self.z[l])\n",
        "            else:\n",
        "              print(\"activation fun not valid!\")\n",
        "\n",
        "          self.z[self.n_layers] = np.dot(self.weights[self.n_layers], self.a[self.n_layers-1])\n",
        "          self.a[self.n_layers] = self.softmax(self.z[self.n_layers])\n",
        "\n",
        "          pred_prob[i] = self.a[self.n_layers].copy()\n",
        "\n",
        "        # return the numpy array y which contains the predicted values\n",
        "        #print(pred_prob)\n",
        "        return pred_prob\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predicting values using the trained linear model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y : 1-dimensional numpy array of shape (n_samples,) which contains the predicted values.\n",
        "        \"\"\"\n",
        "        #get predicted prob\n",
        "        pred_prob = self.predict_proba(X)\n",
        "        pred = np.zeros(len(X))\n",
        "\n",
        "        for i in range(0, len(X)):\n",
        "          prob1 = pred_prob[i]\n",
        "          pred[i] = np.argmax(prob1)\n",
        "\n",
        "        # return the numpy array y which contains the predicted values\n",
        "        return pred\n",
        "\n",
        "    def score(self, X, y):\n",
        "        \"\"\"\n",
        "        Predicting values using the trained linear model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
        "\n",
        "        y : 1-dimensional numpy array of shape (n_samples,) which acts as testing labels.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        acc : float value specifying the accuracy of the model on the provided testing set\n",
        "        \"\"\"\n",
        "        self.a = {}\n",
        "        self.z = {}\n",
        "        self.num_classes=10\n",
        "        #get predictions\n",
        "        pred = self.predict(X)\n",
        "        score = accuracy_score(y, pred)\n",
        "\n",
        "        # return the numpy array y which contains the predicted values\n",
        "        return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS26wvAcptyh"
      },
      "source": [
        "MNIST Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYo1SAn5qKL_"
      },
      "source": [
        "more"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNLKOX7upvpb"
      },
      "outputs": [],
      "source": [
        "! gdown --id 1KlHddR1fMXyzzDoEJ0zIZEnEjwOQh3uN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdIOApxrpVLb"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZBorbs7rq70"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv(\"fashion-mnist_train.csv\")\n",
        "X = dataset.drop([\"label\"], axis=1).to_numpy()\n",
        "Y = dataset[\"label\"].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lScs0dOzxE8J"
      },
      "outputs": [],
      "source": [
        "norm = MinMaxScaler().fit(X_train)\n",
        "X_train = norm.transform(X_train)\n",
        "X_val = norm.transform(X_val)\n",
        "X_test = norm.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "jjWNrwYtocbA",
        "outputId": "760c4839-45fb-407f-d855-6d8e7e492319"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# balanced classes\n",
        "hist = dataset.hist(column='label',legend=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu73cEpnpEL0",
        "outputId": "f44ca0e7-e295-451a-d8be-12cf3e0152da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#no null rows\n",
        "sum(dataset.isnull().sum() == 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcR2lb7lojFv"
      },
      "outputs": [],
      "source": [
        "# 80-10-10 split\n",
        "#test_size = 0.1 as need to have 90% train-validation size and 10% test size\n",
        "X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, test_size=0.1)\n",
        "\n",
        "#test_size=0.1/0.9 as need to have 10% validation set size and 80% train set size\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size=0.1/0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiv6NBeM7JZf"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.astype('float64')\n",
        "X_val = X_val.astype('float64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYnEr3k3XAx1",
        "outputId": "0132e485-330f-47f2-d0ee-a28d6454d9f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dtype('float64')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKql_bX4ocfR"
      },
      "outputs": [],
      "source": [
        "l = 5\n",
        "ls = np.array([len(X[0]), 256, 128, 64, 10])\n",
        "a = 'relu'\n",
        "lr = 0.01\n",
        "wi = 'normal'\n",
        "bs = 1\n",
        "ne = 5\n",
        "nn = MyNeuralNetwork(l,ls,a,lr,wi,bs,ne)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6By6abhock5"
      },
      "outputs": [],
      "source": [
        "nn.fit(X_train,Y_train, X_val, Y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aXefXZi52L2"
      },
      "outputs": [],
      "source": [
        "pk.dump(nn.weights,open('weights.pkl','wb'), protocol=pk.HIGHEST_PROTOCOL)\n",
        "weights = pk.load(open('weights.pkl','rb' ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPvWQTljkfQz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGg5ZbGCokRu"
      },
      "outputs": [],
      "source": [
        "a1 = np.array(list(nn.train_loss.values()))/-9600\n",
        "a2 = np.array(list(nn.val_loss.values()))/-6000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "_auCn_Qjkf0q",
        "outputId": "fa79ac07-5a7f-49ac-bd70-b8d884df0d78"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb98ba19250>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(np.arange(1,26),a1, label='train loss')\n",
        "plt.plot(np.arange(1,26),a2, label='val loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Epochs vs loss for NN')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tehy6wV4qhDe"
      },
      "outputs": [],
      "source": [
        "DF1 = pd.DataFrame(a1)\n",
        "DF2 = pd.DataFrame(a2)\n",
        "DF1.to_csv(\"trainloss_relu.csv\")\n",
        "DF2.to_csv(\"valloss_relu.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwhX1cMN3kMp"
      },
      "outputs": [],
      "source": [
        "#test accuracies - epoch 25\n",
        "nn.weights = pk.load(open('weights1_25.pkl','rb' ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMsn3zd35aUH",
        "outputId": "7ac80260-90e4-430a-8359-21b48d2bacf3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.867"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nn.score(X_test, Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3zZ0fwKE6r2"
      },
      "outputs": [],
      "source": [
        "#test accuracies - epoch 13\n",
        "nn.weights = pk.load(open('weightsrelu_13 (1).pkl','rb' ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKhgZ7ezE7y6",
        "outputId": "b67dd819-92db-41ea-b3aa-aa0b215f5f97"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8413333333333334"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nn.score(X_test, Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ehYg-k3Fho0"
      },
      "outputs": [],
      "source": [
        "lossmodel = pd.read_csv('trainloss_relu (1).csv', header=None)\n",
        "lm = lossmodel[1][1:26]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLym2TXSV3k8"
      },
      "outputs": [],
      "source": [
        "X = X.astype('float64')\n",
        "norm = MinMaxScaler().fit(X)\n",
        "X = norm.transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkpguIAWFrc5",
        "outputId": "d72c3f69-ceb6-4d32-e9b8-329ad4993e9c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8811666666666667\n",
            "[0.534528542801962, 0.4128641356366644, 0.37387261899972224, 0.35035654258411814, 0.33000975831385004, 0.318002386713407, 0.30554919880790604, 0.29443866692361453, 0.28855784223329567, 0.2792210338070179, 0.27131752097197953, 0.26737050675682816, 0.25871877723751424, 0.25470054201195, 0.24674583525982977, 0.24310788786314824, 0.23968947573886662, 0.23705043487179583, 0.2329528769088019, 0.22791806117257993]\n"
          ]
        }
      ],
      "source": [
        "nn.compare_with_mlp(lm, X, Y)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}